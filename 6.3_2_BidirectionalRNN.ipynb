{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Prepare IMDB movie data(vectorized data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = \"H:\\\\Professional Certification\\\\Deep Learning 2 -Udemy-October 2020\\\\imdb.npz\"\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path = imdb_path, num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reversing the Sequences\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x[::-1] for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pads Sequences\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Study(Training and Evaluating an LSTM using reversed sequences)**\n",
    "\n",
    "Understanding the concept of reversing a sequence and feeding it to LSTM (this is not prerequsite step, but study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 109s 5ms/step - loss: 0.5315 - acc: 0.7467 - val_loss: 0.3811 - val_acc: 0.8556\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 116s 6ms/step - loss: 0.3302 - acc: 0.8696 - val_loss: 0.5142 - val_acc: 0.8030\n"
     ]
    }
   ],
   "source": [
    "#epochs = 10 --> takes too much time and change it to 2\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=2, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About Embedding Layer**\n",
    "\n",
    "   * The Embedding layer is a dictionary that maps integer indices (which stand for specific words) to dense vectors\n",
    "   * It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors--> Dictionary lookup\n",
    "   \n",
    "**More Info...**\n",
    "   * The Embedding Layer takes as input a 2D tensor of integers, of shape (samples, sequence_length)\n",
    "       * where each entry is a sequence of integers\n",
    "       * it can embed sequences of variable lengths\n",
    "   * Example \n",
    "       * feed into the Embedding layer in the batches with shapes (32, 10)(batch of 32 sequences of length 10) or \n",
    "           * (64, 15)(batch of 64 sequences of length 15)\n",
    "   * All sequences in a batch must have the same length\n",
    "       * Sequences that are shorter than others should be padded with zeros, and\n",
    "       * Sequences that are longer should be truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(a) : Training and Evaluating a Bidirectional LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About RMSprop**\n",
    "\n",
    "  * It is a gradient based optimization technique used in training neural networks \n",
    "  * It balances the step size(momentum)\n",
    "     * decreasing the step for large gradients to avoid exploding, and\n",
    "     * increasing the step for small gradients to avoid vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 293s 15ms/step - loss: 0.4976 - acc: 0.7642 - val_loss: 0.3698 - val_acc: 0.8486\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 282s 14ms/step - loss: 0.3269 - acc: 0.8754 - val_loss: 0.3259 - val_acc: 0.8760\n"
     ]
    }
   ],
   "source": [
    "#epochs = 10 --> takes too much time and change it to 2\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=2, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2(b) : Training and Evaluating a Bidirectional GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.Bidirectional(layers.GRU(32)))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 305s 15ms/step - loss: 0.5412 - acc: 0.7115 - val_loss: 0.3903 - val_acc: 0.8374\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 277s 14ms/step - loss: 0.3355 - acc: 0.8669 - val_loss: 0.3739 - val_acc: 0.8492\n"
     ]
    }
   ],
   "source": [
    "#epochs = 10 --> takes too much time and change it to 2\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=2, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
